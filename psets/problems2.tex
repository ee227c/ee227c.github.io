\documentclass[12pt]{article}

\usepackage{macros}

\title{Problem Set 2 for EE227C (Spring 2018):\\
 Convex Optimization and Approximation }
\author{Instructor: Moritz Hardt\\
{\small Email: \tt hardt+ee227c@berkeley.edu}\\ ~\\
Graduate Instructor: Max Simchowitz\\
{\small Email: \tt msimchow+ee227c@berkeley.edu}\\ ~\\
}

\begin{document}
%\setenumerate[0]{leftmargin=0pt}
%\setenumerate[1]{leftmargin=50pt}
\setlist[enumerate,1]{leftmargin=0pt,label=\bf(\Alph*)}
\setlist[enumerate,2]{leftmargin=0pt,label=\bf(\Alph{enumi}.\arabic*)}


\maketitle

\section*{Problem 1: Backtracking Line Search}
	Let $f: \R^n \to \R$ be an $m$-strongly convex, $M$-smooth (and thus differentiable) function with global minimum $x^*$. Consider the following algorithm: 

	\begin{algorithm}[H]
	\SetAlgoLined
	\textbf{Input:} Parameters $\alpha \in (0,1/2)$, $\beta \in (0,1)$, $x_0 \in \R^n$\;
	\For{$t = 0,1,2,\dots$}{
	$g_t \leftarrow \nabla f(x_t)$\;
	\For{$k = 0,1,2,\dots$\label{loop}}{
	If ``sufficient decrease'' condition holds
		\begin{eqnarray}
		f(x_t - \beta^k g_t) \le f(x_t) - \alpha \beta^k \cdot \|g_t\|^2~,
		\end{eqnarray}
		set $\eta_t = \beta^k$ and \textbf{break}
	}\
	Set $x_t \leftarrow x_{t-1}-\eta_tg_t$\
	}
	\caption{Backtracking Line Search}
	\end{algorithm}

	\begin{enumerate}
		\item Show that condition $1$ holds for whenever $\beta^k \in (0,1/M]$.
		\item Show that $\eta_t \ge \min\{1,\beta/M\}$. 
		 %
		 Conclude that the loop in Line~\ref{loop} aways terminates.
		\item Using part $b$, show that 
		\begin{eqnarray}
		f(x_t - \eta_t g_t) \le f(x_t) - \alpha \min\{1,\frac{\beta}{M}\}\|\nabla f(x_t)^2\|
		\end{eqnarray}
		\item Show that there is a constant $C = C(\alpha,\beta,M,m) < 1$ such 
		\begin{eqnarray}
		f(x_t - \eta_t g_t) - f(x_*) \le C(\alpha,\beta,M,m) \cdot (f(x_t) - f(x_*))
		\end{eqnarray}
	\end{enumerate}


\section*{Problem 2: Random Descent Directions}

	Let $f: \R^n \to \R$ be an $m$-strongly convex, $M$-smooth (and thus differentiable) function with global minimum $x^*$. 
%
	Consider the following algorithm:

	\begin{algorithm}[H]
	\SetAlgoLined
	\textbf{Input:} Parameters $\alpha \in (0,1/2)$, $\beta \in (0,1)$, $x_0 \in \R^n$\;
	\For{$t = 0,1,2,\dots$}{
	Set $g_t \unifsim \spheren$\;
	Set $\eta_t := \min_{\eta \ge 0} f(x_{t}-\eta g_t)$\;
	Set $x_{t+1} \leftarrow x_{t}-\eta g_t$\;
	}
	\caption{Random Direction Line Search}
	\algorithmfootnote{
	%
	$\spheren:= \{v \in \R^n: \|v\|_2^2 = 1 \}$ denotes the unit sphere in 
	%
	$\R^n$. 
	%
	$g_t \unifsim \spheren $ denotes the unique rotation invariant distribution on the unit sphere. 
	%
	For example, if $h \sim \mathcal{N}(0,I)$, then $h/\|h\|  \unifsim \spheren $}
	\end{algorithm}

\begin{enumerate}
	\item Prove that the above algorithm is a (non-strict) descent method; that is $f(x_t)$ is non-increasing in $t$. Also prove that unless $x_t = x_*$, $f(x_{t+1}) < f(x_t)$ with probability $1/2$. 
	\item Prove that there exists a numerical constant $C$ such that, if 
	\begin{eqnarray}
	t \ge T(\epsilon) := C n \cdot \frac{M}{m} \log (\frac{f(x_0) - f(x^*)}{\epsilon})~,
	\end{eqnarray}
	then $\Exp[f(x_t) - f(x^*)] \le \epsilon$. 
	\item Ammend the stated algorithm to use line search instead of solving for the exactly-optimal step size. To be clear, you don't have access to $\nabla f(x_t)$, all you are allowed to do at round $t$ is the following:
	\begin{enumerate}
		\item Sample \emph{one} direction $g_t \unifsim \spheren$.
		\item Making (finite) function evaluations of the form $f(x_t - \eta g_t)$ for $\eta \in \R$. Ideally, this should be at most logarithmic in problem parameters.
	\end{enumerate}
	State \emph{both} the number of iterations and the number of function evaluations. Are the rates qualitatively similar? \emph{Hint:} Make the method a non-strict descent method, but accept that on some rounds, you might not make any progress, and you can let $x_{t+1} = x_t$. Just ensure that, with constant probability on each round, you make some progress. 
\end{enumerate}
\section*{Problem 3: Sh*t about Quadratics}
	In this problem, you are going to test the sharpness of our upper and lower bounds for quadratics on a randomly generated instance. Fix $n = 500$. We define the distribution over PSD matrix $\cD(\epsilon)$:
	\begin{definition} $\cD(\epsilon)$ is a distribution of matrix $\mathbf{M} = \mathbf{M}^\top$, defined as follows. Let $\mathbf{X} \in \R^{n \times n}$ denote a matrix with i.i.d $\cN(0,1)$ entries. Generate a random vector $\mathbf{u}$ uniformly from the unit sphere. Define the matrix $\mathbf{M} = \frac{1}{\sqrt{2n}}(\mathbf{X} + \mathbf{X}^{\top}) + (1+\epsilon)\mathbf{u}\mathbf{u}^\top$.
	\end{definition}


	Now, for each $\epsilon \in \cS := \{1,.5,.2,.1,.05\}$, do the following
	\begin{enumerate}
		\item Conduct trials $t = 1,2,\dots,10$.
		\begin{enumerate}
		\item Generate $\mathbf{M} \sim \cD(\epsilon)$ as above, and a random vector $\mathbf{v}$ uniformly on the unit sphere. 
		\item Set $\gamma = 2\lambda_{\max}(\mathbf{M}) - \lambda_{2}(\mathbf{M})$, and define the matrix $\mathbf{N} = \gamma I  - \mathbf{M}$. Definally, define the function $\mathbf{f}(x) = \min_{x} x^T \mathbf{N} x - 2 \langle \mathbf{v}, x \rangle$. What is the condition number of $\mathbf{N}$?
		\item Setting $x_0 = 0$, run gradient descent, a heavy-ball method or nesterov method to solve $\min_{x} \mathbf{f}(x)$ for a good number of iterations (use your discretion). You may compute the eigenvalues of $\mathbf{N}$ to tune your step parameters. 
		\item For both gradient descent and heavy-ball, record for each trial iteration $s$, the difference between $\mathbf{f}(x_s) - \min_{x} \mathbf{f}(x)$ for each iteration. 
		\item Using the step sizes, largest/smallest eigenvalues of $\mathbf{N}$, and the initial point $x_0 = 0$, compute a worst case upper bound for $\mathbf{f}(x_s) - \min_{x} \mathbf{f}(x)$ for each iteration $s$ of gradient descent and the heavy ball method.
		\item Run gradient descent, but this time compute the optimality gap unising ``best'' iterate in the Krylov space. That is, compute 
		\begin{eqnarray}
		\min_{x \in \mathrm{span}(x_1,\dots,x_s)}\mathbf{f}(x) - \min_{x} \mathbf{f}(x)
		\end{eqnarray}
		\item After each trial, you should have a list of 5 values for each iterate $s$: an upper bound for gradient descent, the rate actually attained by gradient descent, an upper bound for heavy ball/nesterov, the rate actualy attained by heavy ball/nesterov, and the ``optimal'' krylov algorith,
	\end{enumerate}
	\item For each of the lists above, average all $10$ trials and plot them on the same plot. How sharp are the upper bounds?
	\end{enumerate}




\end{document}
