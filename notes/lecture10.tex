\section{Lecture 10: Stochastic optimization}
The goal in stochastic optimization is to 
minimize functions of the form
\[
    f(x) = \E_{z\sim \cD}g(x,z)\,
\]
which have stochastic component given by a distribution~$\cD.$ In the case where
the distribution has finite support, the function can be written as
\[
    f(x) = \frac{1}{m}\sum_{i=1}^m f_i(x)\,.
\]
To solve these kinds of problems, we examine the stochastic gradient descent
method and some if its many applications.

\subsection{The stochastic gradient method}

Following Robbins-Monro \cite{Robbins&Monro:1951}, we define the stochastic
gradient method as follows. 

\begin{definition}[Stochastic gradient method]
The stochastic gradient method starts from a point $x_0\in\domain$ and proceeds
according to the update rule
\begin{equation*}
    x_{t+1} = x_t - \eta_t\nabla f_{i_t}(x_t) 
\end{equation*}
where $i_t\in\{1,\dots,m\}$ is either selected at random at each step, or cycled
through a random permutation of $\{1,\dots,m\}$.
\end{definition}

Either of the two methods for selecting~$i_t$ above, lead to the fact that
\[
    \E\nabla f_{i_t}(x) = \nabla f(x)
\]
This is also true when $f(x)=\E g(x,z)$ and at each step we update according to
$\nabla g(x,z)$ for randomly drawn~$z\sim\cD.$
%
\subsubsection{Sanity check}

Let us check that on a simple problem that the stochastic gradient descent yields the optimum. Let $p_1, \dots, p_m\in \R^n$, and define $f\colon \R^n \rightarrow \R_+$:
\begin{equation*}
    \forall x\in \R^n,\ f(x) = \frac{1}{2m}\sum_{i=1}^m \|x-p_i\|_2^2
\end{equation*}
Note that 
here $f_i(x) = \frac{1}{2}\|x-p_i\|_2^2$ 
and $\nabla f_i(x) = x-p_i.$ 
Moreover,
\begin{equation*}
    x^* = \argmin_{x\in\R^d} f(x) = \frac{1}{m}\sum_{i=1}^mp_i
\end{equation*}

Now, run SGM with $\eta_t=\frac{1}{t}$ in cyclic order i.e. $i_t = t$ and $x_0=0$:
\begin{align*}
    x_0 &= 0 \\
    x_1 &= 0 - \frac{1}{1}(0-p_1) = p_1 \\
    x_2 &= p_1 - \frac{1}{2}(p_1-p_2) = \frac{p_1+p_2}{2}\\
    \vdots\\
    x_n &= \frac{1}{m}\sum_{i=1}^mp_i = x^*
\end{align*}

\subsection{The Perceptron}

The
\href{https://www.nytimes.com/1958/07/08/archives/new-navy-device-learns-by-doing-psychologist-shows-embryo-of.html}{New
York Times} wrote in 1958 that the Perceptron \cite{Rosenblatt58theperceptron:}
was:
\begin{quote}
\emph{the embryo of an electronic computer that [the Navy] expects will be able
to walk, talk, see, write, reproduce itself and be conscious of its existence.}
\end{quote}

So, let's see.

\begin{definition}[Perceptron] 
Given labeled points~$((x_1, y_1),\dots,(x_m,y_m))\in(\R^n\times\{-1,1\})^m,$ 
and and initial point~$w_0\in\R^n$, the Perceptron is the following algorithm. 
For $i_t\in\{1,\dots,m\}$ selected uniformly at random,
\begin{equation*}
w_{t+1} =w_t(1-\gamma) + \eta \begin{cases}
    y_{i_t}x_{i_t}& \text{if } y_{i_t}\langle w_t, x_{i_t}\rangle <1\\
    0              & \text{otherwise}
\end{cases}
\end{equation*}
\end{definition}

Reverse-engineering the algorithm, the Perceptron is equivalent to running the
SGM on the Support Vector Machine (SVM) objective function.

\begin{definition}[SVM] 

Given labeled points~$((x_1, y_1),\dots,(x_m,y_m))\in(\R^n\times\{-1,1\})^m,$ 
the SVM objective function is:
\begin{equation*}
    f(w) = \frac{1}{n}\sum_{i=1}^m \max(1-y_i\langle w, x_i\rangle,\; 0) + \lambda \|w\|_2^2
\end{equation*}
The loss function $\max(1-z,\;0)$ is known as the Hinge Loss. The extra 
term~$\lambda \|w\|_2^2$ is known as the regularization term.
\end{definition}

\subsection{Empirical risk minimization}
We have two spaces of objects $\mathcal{X}$ and ${\mathcal{Y}},$ where we think
of $\mathcal{X}$ as the space of \emph{instances} or \emph{examples}, and
$\mathcal{Y}$ is a the set of \emph{labels} or \emph{classes}.

Our goal is to \emph{learn} a function $h\colon\mathcal{X}\to\mathcal{Y}$ which
outputs an object $y\in \mathcal{Y}$, given $ x\in \mathcal{X}$.  Assume there
is a joint distribution $\mathcal{D}$ over the space~$\mathcal{X} \times
\mathcal{Y}$ and the training set consists of $m$ instances $S=((x_1, y_1), \ldots,
(x_m,y_m))$ drawn i.i.d.~from $\mathcal{D}$.

We also define a non-negative real-valued loss function $\ell(y^\prime,y)$ to
measure the difference between the prediction $y^\prime$ and the true outcome
$y$.

\begin{definition}
The \emph{risk} of a function $h\colon\mathcal{X}\to\mathcal{Y}$ is defined as 
\begin{equation*}
    R[h] = \mathbb{E}_{(x,y)\sim\mathcal{D}} \ell(h(x),y)\,.
\end{equation*}
\end{definition}
The ultimate goal of a learning algorithm is to find $h^*$ among a class of functions $\mathcal{H}$ that minimizes $R[h]$:
\begin{equation*}
    h^* \in \arg\min_{h \in \mathcal{H}} R[h]
\end{equation*}
In general, the risk $R[h]$ cannot be computed because the joint distribution is unknown. 

Therefore, we instead minimize a proxy objective known as 
\emph{empirical risk} and defined by averaging the loss function over 
the training set:
\begin{equation*}
    R_S[h] = \frac{1}{m} \sum^m_{i=1}\ell(h(x_i),y_i)
\end{equation*}
An empirical risk minimizer is any point $h^* \in \arg\min_{h \in \mathcal{H}}
R_S[h]$.

The stochastic gradient method can be thought of as minimizing the risk
directly, if each example is only used once. In cases where we make multiple
passes over the training set, it is better to think of it as minimizing the
empirical risk, which can give different solutions than minimizing the risk.
We'll develop tools to relate risk and empirical risk in the next lecture.

\subsection{Online Learning}

An interesting variant of this learning setup is called \emph{online learning}.
It arises when we do not have a set of training data, but rather must make
decisions one-by-one.

\paragraph{Taking advice from experts.}
Imagine we have access to the predictions of $n$ experts. 
We start from an initial distribution over experts, given by
weights~$w_1\in\Delta_n = \{ w\in\R^n\colon \sum_i w_i=1, w_i\ge0\}.$

At each step $t = 1, \ldots, T$:
\begin{itemize}
    \item we randomly choose an expert according to~$w_t$
    \item nature deals us a loss function $f_t\in[-1,1]^n,$ specifying for each
expert~$i$ the loss~$f_t[i]$ incurred by the prediction of expert~$i$ at
time~$t.$
    \item we incur the expected loss $\E_{i\sim w_t}f_t[i]=\langle
w_t,f_t\rangle.$
    \item we get to update our distribution to from $w_t$ to $w_{t+1}.$
\end{itemize}

At the end of the day, we measure how well we performed relative to the best
fixed distribution over experts in hindsight. This is called \emph{regret}:
\begin{equation*}
   R = \sum^T_{t = 1} \langle w_{t}, f_t \rangle - \min_{w \in \Delta_n} \sum^{T}_{t = 1} \langle w, f_t \rangle
\end{equation*}
This is a relative benchmark. Small regret does not say that the loss is necessarily
small. It only says that had we played the same strategy at all steps, we
couldn't have done much better even with the benefit of hindsight.

\subsection{Multiplicative weights update}

Perhaps the most important online learning algorithm is the \emph{multiplicative
weights update}. Starting from the uniform distribution~$w_1,$ proceed according
to the following simple update rule for $t>1,$
\begin{align*}
    v_t[i] &= w_{t-1}[i]e^{-\eta f_t[i]} \tag{exponential weights update}\\
    w_t &= v_t/(\textstyle\sum_i v_t[i]) \tag{normalize}
\end{align*}
The question is \textit{how do we bound the regret} of the multiplicative
weights update? We could do a direct analysis, but instead we'll relate
multiplicative weights to gradient descent and use the convergence guarantees
we already know.

\subsection{Multiplicative weights as mirror descent}

Recall that mirror descent requires a mirror map $\phi : \Omega \to R$ over a
domain $\Omega \in \mathbb{R}^n$ where $\phi$ is strongly convex and
continuously differentiable.

The associated projection is
\begin{equation*}
    \Pi^\phi_\domain (y) = \argmin_{x\in\domain} \mathcal{D}_\phi(x,y)
\end{equation*}
where $\mathcal{D}_\phi(x,y)$ is Bregman divergence.
\begin{definition}
The Bregman divergence measures how good the first order approximation of the
function~$\phi$ is:
    \begin{equation*}
        \mathcal{D}_\phi(x,y) = \phi(x) - \phi(y) - \nabla \phi(y) ^\intercal (x-y)
    \end{equation*}
\end{definition}
The mirror descent update rule is:
\begin{align*}
    \nabla \phi(y_{t+1}) &= \nabla \phi (x_t) - \eta g_t \\
    x_{t+1} &=  \Pi^\phi_\domain (y_{t+1})
\end{align*}
where $g_t \in \partial f(x_t).$ 
In the first homework, we proved the following results.
\begin{theorem}
    let $\|\cdot\|$ be arbitrary norm and suppose that $\phi$ is $\alpha$-strongly convex      \textbf{w.r.t.} $\|\cdot\|$ on $\domain$. Suppose that $f_t$ is $L$-lipschitz \textbf{w.r.t.} $\|\cdot\|$, we have:
    \begin{equation*}
        \frac{1}{T}\sum^T_{t=1} f_t(x_t) \leq \frac{ \mathcal{D}_\phi(x^*,x_0)}{T \eta}+ \eta \frac{L^2}{2\alpha}
    \end{equation*}
\end{theorem}


Multiplicative weights are an instance of the Mirror Descent where $\Phi(w)=
\sum_{i=1}^m w_i \log(w_i)$ is the negative entropy function.
We have that
\[
\nabla\Phi(w) =  1 + \log(w),
\]
where the logarithm is elementwise.
The update rule in Mirror Descent becomes:
\begin{align*}
    \nabla\Phi(v_{t+1}) &=\nabla\Phi(w_{t}) - \eta_tf_t \\
    \implies v_{t+1} &= w_te^{-\eta_t f_t}
\end{align*}

Now comes the projection step. The Bregman divergence is, for all $(x,y)\in\domain^2$:
\begin{align*}
    D_{\Phi}(x,y) &= \Phi(x)-\Phi(y) - \nabla\Phi(y)^T(x-y)\,.
\end{align*}
If we write out this expression and simplify, we recover the well-known
\emph{relative entropy}. The projection
\begin{align*}
    \Pi_{\domain}^{\Phi}(y) = \argmin_{x\in\domain}D_{\Phi}(x,y)
\end{align*}
turns out to just correspond to the normalization step in the update rule.

\paragraph{Concrete rate of convergence.}
To get a concrete rate of convergence from the preceeding theorem,
we still need to determine what value of the strong convexity
constant~$\alpha$ we get in our setting. Here, we choose the norm to be the
$\ell_\infty$-norm. It follows from Pinsker's inequality
that $\Phi$ is $1/2$-strongly convex with respect to the $\ell_\infty$-norm.
Moreover, in the $\ell_\infty$-norm all gradients are bounded by~$1,$ since the
loss ranges in~$[1,1].$ Finally, the relative entropy between the initial
uniform distribution any any other distribution is at most $\log(n).$ Putting
these facts together and balancing the step size~$\eta,$ we get the normalized 
regret bound
\[
O\left(\sqrt{\frac{\log n}{T}}\right).
\]
In particular, this shows that the normalized regret of the multiplicative
update rule is vanishing over time.
