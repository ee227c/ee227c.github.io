\documentclass{article}
\usepackage{amsmath,amssymb,amsbsy,amsfonts,amsthm}
\usepackage{listings}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem*{definition*}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem*{lemma*}{Lemma}
\newtheorem{fact}{Fact}
\newtheorem*{fact*}{Fact}
\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem*{corollary*}{Corollary}

\title{Lecture 22: Non-convex constraints} 

\author{Seyed Mehran Mirramezani and Serena Yuan}
\begin{document}
\maketitle


Last time we discussed optimization of a convex function $f$ over non-convex sets. A typical example for such a problem is $\min \| Ax-y\|_2^2$ subject to $\|x\|_0 \leq s$. Another type of non-convex optimization is to minimize a non-convex function over a convex set. A typical example for this kind of non-convex optimization is known as $l_0$-minimization where $\min \| x\|_0$ subject to an affine convex constraint like $Ax=y$. In sum, the $l_0$-minimization is: 

$$\min_{Ax=y} \|x\|_0$$

One option for solving the above non-convex optimization problem is to relax the $l_0$-objective to the convex $l_1$-objective as:

$$\min_{Ax=y} \|x\|_1$$

A variation of the above problem is Basis Pursuit Denoising (BPDN),

\begin{align}
\min ||x||_1 \text{ s.t. } ||Ax - y||_2 \leq \epsilon.
\end{align}


Note that under appropriate assumptions on $A$, $x$ and $y$, we observed that $l_1$-minimization still gives the right answer (RIP, restricted nullspace).
Sparse linear regression has received a lot of attention and there are many related optimization programs like:
BPDN (allows for noise)
$$\min_{\|Ax-y\|_2 \leq s} \|x\|_1$$

Constrained Lasso
$$\min_{\|x\|_1 \leq \lambda }\|Ax-y\|_2^2 $$

Lagrangian Lasso
$$\min \|Ax-y\|_2^2 +  \lambda \|x\|_1$$

In our setting, we have \\

\centering{nonconvex constraint} 
$$\downarrow$$
\centering{cvx relaxation }
$$\downarrow$$
\centering{PGD (Projected Gradient Descent)},

or more directly, nonconvex constraint $\rightarrow$ PGD. 

Here, the Restricted Isometry Property allows optimization over nonconvex sets.


Now, we discuss the Iterative Hard Thresholding (IHT) which is known as a projected gradient descent (PGD) for sparse vectors.

Iterative Hard Thresholding (IHT) is used with Gradient Descent for the following problem,

\begin{align}
f(x) &= \frac{1}{2} || Ax - y||_2^2 \\
\nabla f(x) &= A^{\top}(Ax - y)
\end{align}

If it is possible to directly optimize over nonconvex sets, then we don't need convex relaxation.

We want to show that the algorithm IHT outputs a solution. The algorithm is defined as:

\begin{align*}
\text{IHT}(y, A, t, s) \\
x^1 &\leftarrow 0 \\
\text{ for } &i=1 ,\cdots, t, \\
\tilde{x}^{i+1} &\leftarrow x^i - A^{\top} (Ax - y) \\
x^{i+1} &\leftarrow P_S( \tilde{x}^{i+1} ) \\
\text{ return }& \hat{x} \leftarrow x^{t+1}
\end{align*}

Note that $P_s$ is a projection onto a set of s-sparse vectors. In the rest of the lecture we will see how we should do this projection and how fast the method is.

RIP: matrix $A$ satisfies the $(s,\delta)$-RIP if for all $s$-sparse vectors we have:
$$(1-\delta) \|x\|_2^2 \leq \|Ax\|_2^2 \leq (1+\delta)\|x\|_2^2$$

Consequences: For all supports $S$ of size $s$, we have:
$$\|I-X_s^TX_s\|_2 \leq \delta$$

Setup: \\
$y = Ax^* + e$ \\
$x^*$ is $s$-sparse with support $S^*$ \\ 
$A$ has ($3s,\frac{1}{4})$-RIP \\
$e$ is arbitrary noise.
Then the following holds,
\begin{theorem}
$$\|x^{i+1}-x^*\|_2 \leq \frac{1}{2} \|x^{i}-x^*\|_2 + \max_{|S| \leq 3s} \|A_s^T e\|_2$$
\end{theorem}

\begin{proof}
Let $S_i = Supp. (x^i)$ and $ S\prime = S^{i+1} \cup S^i \cup S^*$ (so $|S \prime \leq 3s|$). Then:

\begin{align*}
\|x^{i+1}-x^*\|_2 &\leq \|x^{i+1} - \tilde x_{\prime S}^{i+1}\|_2 + \|\tilde x_{\prime S}^{i+1} -x^*\|_2\  \tag{ by "triangle inequality"}\\
			 &\leq 2 \|\tilde x_{ S\prime}^{i+1} -x^*\|_2\\ 
			 &= 2 \|x_{ S\prime}^i - A_ { S\prime}^T(Ax^i -y)-x^*\|\\
			 &= 2 \|x^i - A_ {S\prime}^T(A_ { S \prime}x^i -A_ { S\prime}x^*-e)-x^*\|\\
			 &= 2 \|x^i - x^*- A_ { S\prime}^TA_ { S\prime} (x^i-x^*) + A_ {S \prime}^Te\|\\
			 &\leq 2\| I -  A_ {S \prime}^TA_ {S \prime} (x^i-x^*)\|_2 +   2\| A_ {S \prime}^Te\|_2 \\
			 &\leq 2 \delta \| (x^i-x^*)\|_2 +   2 \max_{|S| \leq 3s}\| A_ {S}^Te\|_2
\end{align*}
\end{proof}

\begin{corollary}
$\| \hat x - x^*\|_2 \leq (\frac{1}{2})^t \|x^* \|_2 + 5\|e\|_2$
so $t = \log \frac{\| x^*\|_2}{\epsilon}$ iterations suffice for $\| \hat x - x^*\|_2 \leq \epsilon + 5\|e\|_2$.
 We have a linear rate and PSG, but the analysis looks some what different (no convexity/smoothness) and we did not need a step size. What is going on?
\end{corollary}
Now consider function $f =  \frac{1}{2}\|Ax-y \|_2^2$ and hence we have $\nabla f(x) = A^T(Ax-y)$.

\begin{definition}
Smoothness:
$$f(x+\Delta) \leq f(x) + \langle \nabla f(x), \Delta \rangle + \frac{L}{2} \| \Delta\|_2^2 $$
the above definition s valid for all $\Delta \in \mathbb{R}^d$  
\end{definition}

What does the smoothness mean for the above function $f$?
$$\frac{1}{2}\|A(x+\Delta) - y\|_2^2 \leq \|Ax-y\|_2^2 + \Delta ^T A^T(Ax-y) + \frac{L}{2}\|\delta\|_2^2$$

then:

$$\frac{1}{2}(x+\Delta)^TA^TA(x+\Delta) + \frac{1}{2}y^Ty \leq \frac{1}{2} x^TA^TAx-y^TAx + \frac{1}{2}y^Ty + \Delta ^T A^TAx- \Delta^TA^Ty + \frac{L}{2} \|\Delta\|_2^2 $$

hence:

$$\frac{1}{2}\Delta^TA^TA\Delta \leq \frac{L}{2}\|\Delta\|_2^2$$

as a result:

$$\|A\Delta \|_2^2 \leq L\|\Delta\|_2^2$$

Note that the above inequality is equivalent to being smoothness looks like RIP.
We have similar results for strong convexity.
\begin{definition}
Strong Convexity:
$$f(x+\Delta) \geq f(x) + \langle \nabla f(x), \Delta \rangle + \frac{l}{2} \| \Delta\|_2^2 $$
the above definition s valid for all $\Delta \in \mathbb{R}^d$  
\end{definition}
We can easily conclude that strong convexity is equivalent to $\|A\Delta \|_2^2 \geq l\|\Delta\|_2^2$.

\begin{definition}
Restricted Strong Convexity (RSC):
$$f(x+\Delta) \geq f(x) + \langle \nabla f(x), \Delta \rangle + \frac{l}{2} \| \Delta\|_2^2 $$
the above definition s valid for all $s$-sparse $\Delta$  
\end{definition}
We can say that:
RIP = RSC + RSM with very good ($\approx 1 + \delta$) condition number $L/l$ (hence constant step size $\approx 1/L$).
There is a lot of work on weakening this assumption and further constraints sets.

Convex relaxation: mostly optimal dependence on condition number.
PGD: can be made to work for arbitrary condition number but worse statistical rate.
Can we match convex relaxation with non-convex PGD? Yes!

Given the sparsity condition, it is possible to do hard thresholding in $O(d)$ time. Given the low-rank condition, one can compute the SVD and find the largest singular values and for a $d_1 \times d_2$ matrix, in $O(d_1 d_2 \min( d_1, d_2)).$


\emph{Group Sparsity:}
We are given a family of groups $G_i \subseteq [d]$, support $\text{supp}(x^*) = \cup_{j \in J} G_j$ for some $|J| \leq g.$ This is NP-hard via set cover. 


\emph{Graph Sparsity:}
Given graph $G$ defined on $[d]$ (nodes are indices in $\{ 1, \cdots, d\}$), $\text{supp}(x^*)$ is a connected subgraph in $G$. Here, projection on set is NP-hard (Steiner trees). 


\emph{Approximate Projection}:
(Tail approximation) Given input $\tilde{x} \in \mathbb{R}^d$, find $x \in C$ ($C$ is the constraint set) such that 
\begin{align*}
||x - \tilde{x} ||_2 \leq \alpha \min_{x' \in C} || x'- \tilde{x}||_2. 
\end{align*}

For $x^*$ that is $1$-sparse and the problem
\begin{align*}
y = Ax^*,
\end{align*}

$A = \pm \frac{1}{\sqrt{n}}$ gives $n= O(\log d)$ and then we have RIP and PGD should work. 

In the first iteration of PGD, 
\begin{center}
$x' \leftarrow 0$ \\
$x^2 \leftarrow \hat{P}_S(A^{\top} y)$
\end{center}


Consider the following setup. 

$$b_1 = 1$$

$$\mathbb{E}[b_i^2] = \frac{1}{n}$$

$$\mathbb{E}[ ||b||_2^2] = 1 + \frac{d-1}{n}$$.


The approximate error of the best projection is on the order $\frac{d-1}{n}$.

\begin{definition}
Head Projection:

Given $\tilde{x} \in \mathbb{R}^d,$ find a support $S$ such that
\begin{align}
||\tilde{x}_S||_2 \geq \beta \max_{S' \in \text{Supp}(C)} ||\tilde{x}_{S'}||_2.
\end{align}
\end{definition}

When combined with approximate projection, PGD still works.



\end{document}
